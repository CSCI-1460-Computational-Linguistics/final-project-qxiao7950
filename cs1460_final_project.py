# -*- coding: utf-8 -*-
"""CS1460 final project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TLx9RlrQpEzLxC0Calh5kLSNWu68VfEn

# Reddit Depression Final Project
Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621

Read through the paper fully before starting the assignment!
"""

import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_validate, cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier

from google.colab import drive
drive.mount('/content/drive')

FILEPATH = 'drive/MyDrive/Colab Notebooks/CS1460 final project/student.pkl'

"""## Preprocessing"""

def load(filepath: str):
  """Load pickles"""
  """Args:
      filepath (str): path to the pickle file
      Returns:
      data (pd dataframe): loaded pickle file
  """

  data = pd.read_pickle(FILEPATH)

  return data

  #pass

data = load(FILEPATH)
print(data.head())
print(data.info())

# List of depression subreddits in the paper
depression_subreddits = ["Anger",
    "anhedonia", "DeadBedrooms",
    "Anxiety", "AnxietyDepression", "HealthAnxiety", "PanicAttack",
    "DecisionMaking", "shouldi",
    "bingeeating", "BingeEatingDisorder", "EatingDisorders", "eating_disorders", "EDAnonymous",
    "chronicfatigue", "Fatigue",
    "ForeverAlone", "lonely",
    "cry", "grief", "sad", "Sadness",
    "AvPD", "SelfHate", "selfhelp", "socialanxiety", "whatsbotheringyou",
    "insomnia", "sleep",
    "cfs", "ChronicPain", "Constipation", "EssentialTremor", "headaches", "ibs", "tinnitus",
    "AdultSelfHarm", "selfharm", "SuicideWatch",
    "Guilt", "Pessimism", "selfhelp", "whatsbotheringyou"
]

loneliness_subreddits = ["ForeverAlone", "lonely"]

def dataset_generation(data, depression_subreddits, time_gap = 180):
  """Build control and symptom datasets"""
  """Args:
      data (pd dataframe): full dataset
      depression_subreddits (list[str]): subreddits with posts and themes representing depression symptoms
      time_gap (int): min number of days between control and symptom posts by some given user
      Returns:
      symptom_data (pd dataframe): symptom dataset
      control_data (pd dataframe): control dataset
  """

  time_gap_seconds = time_gap * 24 * 60 * 60
  symptom_data = data[data['subreddit'].isin(depression_subreddits)]
  print(f"Symptom dataset size: {symptom_data.shape[0]}")

  earliest_symptom_times = symptom_data.groupby('author')['created_utc'].min()

  control_data = data[
      (data['subreddit'].isin(depression_subreddits) == False) &
      (data['author'].isin(earliest_symptom_times.index)) &
      (data['created_utc'] <= data['author'].map(earliest_symptom_times) - time_gap_seconds)
  ]
  print(f"Control dataset size: {control_data.shape[0]}")

  return symptom_data, control_data



  #pass

symptoms_to_subreddits = {
    "anger": ["Anger"],
    "anhedonia": ["anhedonia", "DeadBedrooms"],
    "anxiety": ["Anxiety", "AnxietyDepression", "HealthAnxiety", "PanicAttack"],
    "concentration_deficit": ["DecisionMaking", "shouldi"],
    "disordered_eating": ["bingeeating", "BingeEatingDisorder", "EatingDisorders", "eating_disorders", "EDAnonymous"],
    "fatigue": ["chronicfatigue", "Fatigue"],
    "loneliness": ["ForeverAlone", "lonely"],
    "sad_mood": ["cry", "grief", "sad", "Sadness"],
    "self_loathing": ["AvPD", "SelfHate", "selfhelp", "socialanxiety", "whatsbotheringyou"],
    "sleep_problem": ["insomnia", "sleep"],
    "somatic_complaint": ["cfs", "ChronicPain", "Constipation", "EssentialTremor", "headaches", "ibs", "tinnitus"],
    "suicidal_thoughts_and_attempts": ["AdultSelfHarm", "selfharm", "SuicideWatch"],
    "worthlessness": ["Guilt", "Pessimism", "selfhelp", "whatsbotheringyou"]

}

def dataset_generation_specified(data, depression_subreddits, symptoms_to_subreddits, time_gap = 180):
  """Build control and symptom datasets--13 smaller datasets for each symptom instead of one umbrella set as in the other version of the function"""
  """Args:
      data (pd dataframe): full dataset
      depression_subreddits (list[str]): subreddits with posts and themes representing depression symptoms
      symptoms_to_subreddits (dict): key: symptom name, value: list of subreddits pertaining to that symptom
      time_gap (int): min number of days between control and symptom posts by some given user
      Returns:
      symptom_data (pd dataframe): symptom dataset
      control_data (pd dataframe): control dataset
  """

  time_gap_seconds = time_gap * 24 * 60 * 60
  all_symptom_data = data[data['subreddit'].isin(depression_subreddits)]
  print(f"Symptom dataset size: {all_symptom_data.shape[0]}")

  earliest_symptom_times = all_symptom_data.groupby('author')['created_utc'].min()

  control_data = data[
      (data['subreddit'].isin(depression_subreddits) == False) &
      (data['author'].isin(earliest_symptom_times.index)) &
      (data['created_utc'] <= data['author'].map(earliest_symptom_times) - time_gap_seconds)
  ]
  print(f"Control dataset size: {control_data.shape[0]}")

  symptom_datasets = {}
  for symptom, subreddits in symptoms_to_subreddits.items():
    symptom_data = all_symptom_data[all_symptom_data['subreddit'].isin(subreddits)]
    symptom_datasets[symptom] = symptom_data
    print(f"{symptom} dataset size: {symptom_data.shape[0]}")

  return symptom_datasets, control_data

symptom_data, control_data = dataset_generation(data, depression_subreddits)
# save them to files in drive
symptom_data.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/symptom_data.pkl')
control_data.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_data.pkl')

# same as above but for specific symptoms
symptom_datasets, control_data = dataset_generation_specified(data, depression_subreddits, symptoms_to_subreddits)
for symptom, symptom_data in symptom_datasets.items():
  symptom_data.to_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_data.pkl')
control_data.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_data1.pkl')

!pip install happiestfuntokenizing
from happiestfuntokenizing.happiestfuntokenizing import Tokenizer

def tokenize(data):
  """Tokenize"""
  """Args:
      data (pd dataframe): dataset
      Returns:
      tokenized_data (pd dataframe): dataset with tokenized text
  """

  tokenizer = Tokenizer()

  tokenized_data = data.copy()
  tokenized_data['tokenized_text'] = tokenized_data['text'].apply(tokenizer.tokenize)

  return tokenized_data


  #pass

def further_cleaning(data):
  """Further cleaning"""
  """Args:
      data (pd dataframe): dataset
      Returns:
      cleaned_data (pd dataframe): dataset with further cleaning
  """

  cleaned_data = data.copy()

  cleaned_data['tokenized_text'] = cleaned_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])
  cleaned_data['tokenized_text'] = cleaned_data['tokenized_text'].apply(lambda x: [word for word in x if word.isalnum()])

  return cleaned_data

#data_tokenized = tokenize(data)
#data_tokenized.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/data_tokenized.pkl')

control_data_tokenized = tokenize(control_data)
control_data_tokenized = further_cleaning(control_data_tokenized)
control_data_tokenized.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_data_tokenized.pkl')

symptom_data_tokenized = tokenize(symptom_data)
symptom_data_tokenized = further_cleaning(symptom_data_tokenized)
symptom_data_tokenized.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/symptom_data_tokenized.pkl')

print(control_data_tokenized.head())

tokenized_symptom_data = dict()

for symptom, symptom_data in symptom_datasets.items():
  symptom_data_tokenized = tokenize(symptom_data)
  symptom_data_tokenized = further_cleaning(symptom_data_tokenized)
  symptom_data_tokenized.to_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_data_tokenized.pkl')
  tokenized_symptom_data[symptom] = symptom_data_tokenized

print(symptom_data_tokenized.head())

from collections import Counter

def stop_words(data):
  """Find top 100 words from Reddit dataset to use as stop words"""
  """Args:
      data (pd dataframe): dataset
      Returns:
      stop_words (list[str]): list of stop words
  """

  stop_words = []

  all_tokens = [token for token in data['tokenized_text']]
  all_tokens = [token for sublist in all_tokens for token in sublist]

  token_counts = Counter(all_tokens)
  output_stop_words = [token for token, count in token_counts.most_common(100)]

  return output_stop_words

  #pass

control_stop_words = stop_words(control_data_tokenized)
print(control_stop_words)

def remove_stop_words(data, stop_words):
  """Remove stop words from dataset"""
  """Args:
      data (pd dataframe): dataset
      stop_words (list[str]): list of stop words
      Returns:
      data_without_stop_words (pd dataframe): dataset without stop words
  """
  data_without_stop_words = data.copy()
  data_without_stop_words['tokenized_text'] = data_without_stop_words['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])

  return data_without_stop_words

control_data_without_stop_words = remove_stop_words(control_data_tokenized, control_stop_words)
symptom_data_without_stop_words = remove_stop_words(symptom_data_tokenized, control_stop_words)

print(control_data_without_stop_words.head())
print(symptom_data_without_stop_words.head())

# split data up into several pieces and run methods
# I just realized I don't think I actually need this

def split_data(data, n):
  """Split data into n pieces"""
  """Args:
      data (pd dataframe): dataset
      n (int): number of pieces to split data into
      Returns:
      data_pieces (list[pd dataframe]): list of n pieces of data
  """

  return np.array_split(data, n)

def process_chunks(data_chunks):
  """Process chunks of data using the preprocessing methods I wrote above"""
  """Args:
      data_chunks (list[pd dataframe]): list of n pieces of data
      Returns:
      data_without_stop_words (list[pd dataframe]): list of n pieces of data without stop words
  """

  processed_chunks = []
  for chunk in data_chunks:
    chunk = tokenize(chunk)
    chunk = further_cleaning(chunk)
    chunk = remove_stop_words(chunk, control_stop_words)
    processed_chunks.append(chunk)

  return processed_chunks

def combine_chunks(data_chunks):
  """Combine chunks of data into one dataset"""
  """Args:
      data_chunks (list[pd dataframe]): list of n pieces of data
      Returns:
      combined_data (pd dataframe): combined dataset
  """

  combined_data = pd.concat(data_chunks)
  return combined_data

split_original_data = split_data(data, 10)
cleaned_original_data = process_chunks(split_original_data)
combined_original_data = combine_chunks(cleaned_original_data)

# I don't think I actually need this, nvm!

combined_data_without_stop_words = pd.concat([control_data_without_stop_words, symptom_data_without_stop_words])
print(combined_data_without_stop_words.head())

# I spent an embarrassingly long time trying to figure out how to combine two
# gensim Dictionary objects together into another Dictionary before I realized
# I could just do it up here beforehand

"""## Reddit Topics with LDA

 - Don't use MALLET (as the paper does), use some other LDA implementation.
"""

# We highly recommend you using the LdaMulticore interface, but feel free to use any other implementations if you prefer.
# from gensim.models import LdaMulticore

# TODO: Your LDA code!

from gensim.corpora import Dictionary
from gensim.models import LdaMulticore

dictionary = Dictionary(combined_data_without_stop_words['tokenized_text'])
print(f"Number of unique tokens: {len(dictionary)}")

corpus = [dictionary.doc2bow(text) for text in control_data_without_stop_words['tokenized_text']]
print(f"Corpus size: {len(dictionary)}")

dictionary.save('control_dict.gensim')
with open('control_corpus.pkl', 'wb') as f:
  pickle.dump(corpus, f)

def train_lda_model(dictionary, corpus, num_topics=200, alpha=5, passes=10, workers=4):
    """Train LDA model"""
    """Args:
        dictionary: gensim dictionary that maps words to IDs
        corpus: BoW corpus
        num_topics: number of topics
        alpha: hyperparameter for sparsity of topics per doc
        passes: number of passes
        workers: number of workers
        Returns:
        lda_model: trained LDA model
        topics: list of topics
    """

    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha=alpha/num_topics, passes=passes, workers=workers)
    topic_features = []
    for doc in corpus:
      topics = lda_model.get_document_topics(doc)
      topic_features.append([prob for topic, prob in topics])

    topic_features = pd.DataFrame(topic_features)

    return lda_model, topics

lda_model, topics = train_lda_model(dictionary, corpus)

lda_model.save('lda_model.gensim')

print(lda_model.print_topics())

# not too sure how to interpret this but ok

for symptom in tokenized_symptom_data:
  symptom_corpus = [dictionary.doc2bow(text) for text in tokenized_symptom_data[symptom]['tokenized_text']]
  symptom_topic_features = []
  for doc in symptom_corpus:
    topics = lda_model.get_document_topics(doc, minimum_probability=0.0000)
    symptom_topic_features.append([prob for topic, prob in topics])

  symptom_topic_features = pd.DataFrame(symptom_topic_features, columns=[f'topic_{i}' for i in range(200)])
  symptom_topic_features.to_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_topic_features.pkl')

control_corpus = [dictionary.doc2bow(text) for text in control_data_without_stop_words['tokenized_text']]
control_topic_features = []
for doc in control_corpus:
  topics = lda_model.get_document_topics(doc, minimum_probability=0.0000)
  control_topic_features.append([prob for topic, prob in topics])

control_topic_features = pd.DataFrame(control_topic_features, columns=[f'topic_{i}' for i in range(200)])
control_topic_features.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_topic_features.pkl')

symptom_topic_features_df = pd.read_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/anxiety_topic_features.pkl')
print(symptom_topic_features_df.head())
print(symptom_topic_features.shape)
print(control_topic_features.head())
print(control_topic_features.shape)

"""## RoBERTa Embeddings"""

# TODO: Your RoBERTa code!

from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("distilroberta-base")
model = AutoModel.from_pretrained("distilroberta-base", output_hidden_states=True)

model.eval()

def get_roberta_embeddings(text_list, tokenizer, model, layer_index=5, max_length=512, device='cuda' if torch.cuda.is_available() else 'cpu'):
  """Get RoBERTa embeddings"""
  """Args:
      text_list: list of strings
      tokenizer: tokenizer
      model: model
      layer_index: index of layer to extract embeddings from
      max_length: max length of tokens
      device: device
      Returns:
      embeddings: list of embeddings
  """
  model.to(device)

  embeddings = []

  for text in text_list:
    inputs = tokenizer(text, return_tensors="pt", padding=True, max_length=max_length, truncation=True).to(device)
    with torch.no_grad():
      outputs = model(**inputs)

    hidden_states = outputs.hidden_states
    layer_hidden_state = hidden_states[layer_index]

    batch_embeddings = layer_hidden_state.mean(dim=1).squeeze().cpu().numpy()
    embeddings.append(batch_embeddings)

  return embeddings

control_texts = control_data_without_stop_words['tokenized_text'].apply(lambda x: ' '.join(x)).tolist()
for symptom in tokenized_symptom_data:
  symptom_texts = tokenized_symptom_data[symptom]['tokenized_text'].apply(lambda x: ' '.join(x)).tolist()
  symptom_embeddings = get_roberta_embeddings(symptom_texts, tokenizer, model)
  np.save(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_roberta_embeddings.npy', symptom_embeddings)

control_embeddings = get_roberta_embeddings(control_texts, tokenizer, model)


np.save('control_embeddings.npy', control_embeddings)

"""## Main"""

def main(X, y, symptom_name):
  """
  Here's the basic structure of the main block! It should run
  5-fold cross validation with random forest to evaluate your RoBERTa and LDA
  performance.
  """
  """Args:
      X (np array): features
      y (np array): labels
      symptom_name (str): name of the symptom
      Returns:
      None
  """

  rf_classifier = RandomForestClassifier()
  cv = KFold(n_splits=5, shuffle=True)
  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)

  # TODO: Print your training and testing scores!

  print(f"Results for symptom: {symptom_name}")
  print(f"Training AUC Scores: {results['train_score']}")
  print(f"Mean Training AUC: {results['train_score'].mean():.4f}")
  print(f"Testing AUC Scores: {results['test_score']}")
  print(f"Mean Testing AUC: {results['test_score'].mean():.4f}\n")


  #pass

for symptom_name in symptoms_to_subreddits:
  symptom_topic_features_df = pd.read_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom_name}_topic_features.pkl')
  control_features_df = pd.read_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_topic_features.pkl')

  control_features_df['label'] = 0
  symptom_topic_features_df['label'] = 1

  combined_features_df = pd.concat([control_features_df, symptom_topic_features_df])

  X = combined_features_df.drop(columns=['label']).values
  y = combined_features_df['label'].values

  print(f"Evaluating symptom: {symptom_name}")
  main(X, y, symptom_name)

for symptom_name in symptoms_to_subreddits:
  symptom_roberta_embeddings = np.load(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom_name}_roberta_embeddings.npy')
  control_roberta_embeddings = np.load(f'drive/MyDrive/Colab Notebooks/CS1460 final project/control_embeddings.npy')

  X = np.concatenate([symptom_roberta_embeddings, control_roberta_embeddings])
  y = np.concatenate([np.ones(symptom_roberta_embeddings.shape[0]), np.zeros(control_roberta_embeddings.shape[0])])

  main(X, y, symptom_name)