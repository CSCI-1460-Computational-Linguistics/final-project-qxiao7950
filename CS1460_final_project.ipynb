{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Depression Final Project\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ],
      "metadata": {
        "id": "9jFvbbC6VtZm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FoBxKQ_OVl-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfe4249-6641-458d-d746-b16f11a8afaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = 'drive/MyDrive/Colab Notebooks/CS1460 final project/student.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "rcMOTL7mV9T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(filepath: str):\n",
        "  \"\"\"Load pickles\"\"\"\n",
        "  \"\"\"Args:\n",
        "      filepath (str): path to the pickle file\n",
        "      Returns:\n",
        "      data (pd dataframe): loaded pickle file\n",
        "  \"\"\"\n",
        "\n",
        "  data = pd.read_pickle(FILEPATH)\n",
        "\n",
        "  return data\n",
        "\n",
        "  #pass"
      ],
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load(FILEPATH)\n",
        "print(data.head())\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo04euEcjmZD",
        "outputId": "2696a9fc-b72d-4497-e546-63043dbcc9d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text            author  \\\n",
            "0  does your life feel like a waste mines not a c...        trademeple   \n",
            "1  Just relapsed again. Any advice I just got to ...          kenny818   \n",
            "2  Audio and mic not working? So I have a HyperX ...          psyjinks   \n",
            "3  PG&amp;E: Mylar balloon causes outage in centr...            Majnum   \n",
            "4                                    Um... Forward?   OldManoftheNorth   \n",
            "\n",
            "     subreddit  created_utc     date  \n",
            "0   depression   1504920055  2017-09  \n",
            "1        NoFap   1507890053  2017-10  \n",
            "2  techsupport   1513558467  2017-12  \n",
            "3  nottheonion   1499573023  2017-07  \n",
            "4        memes   1516842851  2018-01  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1958158 entries, 0 to 1969753\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Dtype \n",
            "---  ------       ----- \n",
            " 0   text         object\n",
            " 1   author       object\n",
            " 2   subreddit    object\n",
            " 3   created_utc  int64 \n",
            " 4   date         object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 89.6+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]"
      ],
      "metadata": {
        "id": "ohOK3wCdWpnA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loneliness_subreddits = [\"ForeverAlone\", \"lonely\"]"
      ],
      "metadata": {
        "id": "9ckfHMZ1m8IU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_generation(data, depression_subreddits, time_gap = 180):\n",
        "  \"\"\"Build control and symptom datasets\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data (pd dataframe): full dataset\n",
        "      depression_subreddits (list[str]): subreddits with posts and themes representing depression symptoms\n",
        "      time_gap (int): min number of days between control and symptom posts by some given user\n",
        "      Returns:\n",
        "      symptom_data (pd dataframe): symptom dataset\n",
        "      control_data (pd dataframe): control dataset\n",
        "  \"\"\"\n",
        "\n",
        "  time_gap_seconds = time_gap * 24 * 60 * 60\n",
        "  symptom_data = data[data['subreddit'].isin(depression_subreddits)]\n",
        "  print(f\"Symptom dataset size: {symptom_data.shape[0]}\")\n",
        "\n",
        "  earliest_symptom_times = symptom_data.groupby('author')['created_utc'].min()\n",
        "\n",
        "  control_data = data[\n",
        "      (data['subreddit'].isin(depression_subreddits) == False) &\n",
        "      (data['author'].isin(earliest_symptom_times.index)) &\n",
        "      (data['created_utc'] <= data['author'].map(earliest_symptom_times) - time_gap_seconds)\n",
        "  ]\n",
        "  print(f\"Control dataset size: {control_data.shape[0]}\")\n",
        "\n",
        "  return symptom_data, control_data\n",
        "\n",
        "\n",
        "\n",
        "  #pass"
      ],
      "metadata": {
        "id": "Wpw9kJiras4B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symptoms_to_subreddits = {\n",
        "    \"anger\": [\"Anger\"],\n",
        "    \"anhedonia\": [\"anhedonia\", \"DeadBedrooms\"],\n",
        "    \"anxiety\": [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"],\n",
        "    \"concentration_deficit\": [\"DecisionMaking\", \"shouldi\"],\n",
        "    \"disordered_eating\": [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"],\n",
        "    \"fatigue\": [\"chronicfatigue\", \"Fatigue\"],\n",
        "    \"loneliness\": [\"ForeverAlone\", \"lonely\"],\n",
        "    \"sad_mood\": [\"cry\", \"grief\", \"sad\", \"Sadness\"],\n",
        "    \"self_loathing\": [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"],\n",
        "    \"sleep_problem\": [\"insomnia\", \"sleep\"],\n",
        "    \"somatic_complaint\": [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"],\n",
        "    \"suicidal_thoughts_and_attempts\": [\"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\"],\n",
        "    \"worthlessness\": [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "tcAjFqEN5ZAb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_generation_specified(data, depression_subreddits, symptoms_to_subreddits, time_gap = 180):\n",
        "  \"\"\"Build control and symptom datasets--13 smaller datasets for each symptom instead of one umbrella set as in the other version of the function\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data (pd dataframe): full dataset\n",
        "      depression_subreddits (list[str]): subreddits with posts and themes representing depression symptoms\n",
        "      symptoms_to_subreddits (dict): key: symptom name, value: list of subreddits pertaining to that symptom\n",
        "      time_gap (int): min number of days between control and symptom posts by some given user\n",
        "      Returns:\n",
        "      symptom_data (pd dataframe): symptom dataset\n",
        "      control_data (pd dataframe): control dataset\n",
        "  \"\"\"\n",
        "\n",
        "  time_gap_seconds = time_gap * 24 * 60 * 60\n",
        "  all_symptom_data = data[data['subreddit'].isin(depression_subreddits)]\n",
        "  print(f\"Symptom dataset size: {all_symptom_data.shape[0]}\")\n",
        "\n",
        "  earliest_symptom_times = all_symptom_data.groupby('author')['created_utc'].min()\n",
        "\n",
        "  control_data = data[\n",
        "      (data['subreddit'].isin(depression_subreddits) == False) &\n",
        "      (data['author'].isin(earliest_symptom_times.index)) &\n",
        "      (data['created_utc'] <= data['author'].map(earliest_symptom_times) - time_gap_seconds)\n",
        "  ]\n",
        "  print(f\"Control dataset size: {control_data.shape[0]}\")\n",
        "\n",
        "  symptom_datasets = {}\n",
        "  for symptom, subreddits in symptoms_to_subreddits.items():\n",
        "    symptom_data = all_symptom_data[all_symptom_data['subreddit'].isin(subreddits)]\n",
        "    symptom_datasets[symptom] = symptom_data\n",
        "    print(f\"{symptom} dataset size: {symptom_data.shape[0]}\")\n",
        "\n",
        "  return symptom_datasets, control_data"
      ],
      "metadata": {
        "id": "N4JCwabE4rGK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symptom_data, control_data = dataset_generation(data, depression_subreddits)\n",
        "# save them to files in drive\n",
        "symptom_data.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/symptom_data.pkl')\n",
        "control_data.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_data.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uirjBfH7sMoE",
        "outputId": "bd126da6-3d5a-4afd-a283-1c4ffa34561a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symptom dataset size: 94514\n",
            "Control dataset size: 4369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same as above but for specific symptoms\n",
        "symptom_datasets, control_data = dataset_generation_specified(data, depression_subreddits, symptoms_to_subreddits)\n",
        "for symptom, symptom_data in symptom_datasets.items():\n",
        "  symptom_data.to_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_data.pkl')\n",
        "control_data.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_data1.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehCnGmJu6dzb",
        "outputId": "2aba2129-0440-433e-9603-259db72bc44c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symptom dataset size: 94514\n",
            "Control dataset size: 4369\n",
            "anger dataset size: 555\n",
            "anhedonia dataset size: 5934\n",
            "anxiety dataset size: 24514\n",
            "concentration_deficit dataset size: 10\n",
            "disordered_eating dataset size: 1789\n",
            "fatigue dataset size: 1\n",
            "loneliness dataset size: 11535\n",
            "sad_mood dataset size: 2222\n",
            "self_loathing dataset size: 9865\n",
            "sleep_problem dataset size: 3184\n",
            "somatic_complaint dataset size: 8330\n",
            "suicidal_thoughts_and_attempts dataset size: 26520\n",
            "worthlessness dataset size: 1805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happiestfuntokenizing\n",
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSU1y0uC7Ud1",
        "outputId": "c7987fe2-8842-4a21-b55a-0b01ba7ff175"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting happiestfuntokenizing\n",
            "  Downloading happiestfuntokenizing-0.0.7.tar.gz (6.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: happiestfuntokenizing\n",
            "  Building wheel for happiestfuntokenizing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for happiestfuntokenizing: filename=happiestfuntokenizing-0.0.7-py3-none-any.whl size=6711 sha256=476757eb702f1667a3775b80dbe7afeb5ef667d2d60a7920eb8de2827fd13a35\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/c9/4d/310f0c60855eb7b428558f29d93cf464dbb64c1b8628753395\n",
            "Successfully built happiestfuntokenizing\n",
            "Installing collected packages: happiestfuntokenizing\n",
            "Successfully installed happiestfuntokenizing-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(data):\n",
        "  \"\"\"Tokenize\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data (pd dataframe): dataset\n",
        "      Returns:\n",
        "      tokenized_data (pd dataframe): dataset with tokenized text\n",
        "  \"\"\"\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "\n",
        "  tokenized_data = data.copy()\n",
        "  tokenized_data['tokenized_text'] = tokenized_data['text'].apply(tokenizer.tokenize)\n",
        "\n",
        "  return tokenized_data\n",
        "\n",
        "\n",
        "  #pass\n",
        "\n",
        "def further_cleaning(data):\n",
        "  \"\"\"Further cleaning\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data (pd dataframe): dataset\n",
        "      Returns:\n",
        "      cleaned_data (pd dataframe): dataset with further cleaning\n",
        "  \"\"\"\n",
        "\n",
        "  cleaned_data = data.copy()\n",
        "\n",
        "  cleaned_data['tokenized_text'] = cleaned_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])\n",
        "  cleaned_data['tokenized_text'] = cleaned_data['tokenized_text'].apply(lambda x: [word for word in x if word.isalnum()])\n",
        "\n",
        "  return cleaned_data"
      ],
      "metadata": {
        "id": "MWGVUju_WxuP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_tokenized = tokenize(data)\n",
        "#data_tokenized.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/data_tokenized.pkl')\n",
        "\n",
        "control_data_tokenized = tokenize(control_data)\n",
        "control_data_tokenized = further_cleaning(control_data_tokenized)\n",
        "control_data_tokenized.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_data_tokenized.pkl')\n",
        "\n",
        "symptom_data_tokenized = tokenize(symptom_data)\n",
        "symptom_data_tokenized = further_cleaning(symptom_data_tokenized)\n",
        "symptom_data_tokenized.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/symptom_data_tokenized.pkl')"
      ],
      "metadata": {
        "id": "qL0JwKnf-4YM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(control_data_tokenized.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zTnCrtjvvAW",
        "outputId": "585ad308-2488-4064-f62c-103193ef2e59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text                author  \\\n",
            "315            Man, I do love me some Bandicoot crash.    BuddermanTheAmazing   \n",
            "651   How good is this PC for my 700-750$ budget? Wa...   WildernessExploring   \n",
            "730   When is the price of gpus going down? I know t...  NeighborhoodPizzaGuy   \n",
            "1354  Our service is not available in your area. Hey...               xDEDANx   \n",
            "1598                                               Wow            baby_kicked   \n",
            "\n",
            "                 subreddit  created_utc     date  \\\n",
            "315        crappyoffbrands   1499236239  2017-07   \n",
            "651             buildmeapc   1501296261  2017-07   \n",
            "730           pcmasterrace   1500082729  2017-07   \n",
            "1354                  njpw   1499941432  2017-07   \n",
            "1598  indianpeoplefacebook   1500924182  2017-07   \n",
            "\n",
            "                                         tokenized_text  \n",
            "315      [man, i, do, love, me, some, bandicoot, crash]  \n",
            "651   [how, good, is, this, pc, for, my, budget, wan...  \n",
            "730   [when, is, the, price, of, gpus, going, down, ...  \n",
            "1354  [our, service, is, not, available, in, your, a...  \n",
            "1598                                              [wow]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_symptom_data = dict()\n",
        "\n",
        "for symptom, symptom_data in symptom_datasets.items():\n",
        "  symptom_data_tokenized = tokenize(symptom_data)\n",
        "  symptom_data_tokenized = further_cleaning(symptom_data_tokenized)\n",
        "  symptom_data_tokenized.to_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_data_tokenized.pkl')\n",
        "  tokenized_symptom_data[symptom] = symptom_data_tokenized"
      ],
      "metadata": {
        "id": "fsayhJIGFGpH"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(symptom_data_tokenized.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqkCyngVwxQg",
        "outputId": "cd803f1d-03de-47c3-f258-0616c0b2d837"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text         author  \\\n",
            "1640  Would you Rather Be Superman Or Have a Massive...        torero5   \n",
            "3187  I have basically everything I’ve really wanted...    Wandertramp   \n",
            "3842  how do i cope with defeat and failure? so i ju...  that_one_kid9   \n",
            "4273  How could I be bored with so much work to do? ...        Sraktai   \n",
            "5346  How to eat decent meals I have been struggling...      MrAfr1can   \n",
            "\n",
            "              subreddit  created_utc     date  \\\n",
            "1640           selfhelp   1514386325  ression   \n",
            "3187           selfhelp   1508588646  ression   \n",
            "3842           selfhelp   1503614431  ression   \n",
            "4273  whatsbotheringyou   1507628776  ression   \n",
            "5346           selfhelp   1500861393  ression   \n",
            "\n",
            "                                         tokenized_text  \n",
            "1640  [would, you, rather, be, superman, or, have, a...  \n",
            "3187  [i, have, basically, everything, i, ve, really...  \n",
            "3842  [how, do, i, cope, with, defeat, and, failure,...  \n",
            "4273  [how, could, i, be, bored, with, so, much, wor...  \n",
            "5346  [how, to, eat, decent, meals, i, have, been, s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def stop_words(data):\n",
        "  \"\"\"Find top 100 words from Reddit dataset to use as stop words\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data (pd dataframe): dataset\n",
        "      Returns:\n",
        "      stop_words (list[str]): list of stop words\n",
        "  \"\"\"\n",
        "\n",
        "  stop_words = []\n",
        "\n",
        "  all_tokens = [token for token in data['tokenized_text']]\n",
        "  all_tokens = [token for sublist in all_tokens for token in sublist]\n",
        "\n",
        "  token_counts = Counter(all_tokens)\n",
        "  output_stop_words = [token for token, count in token_counts.most_common(100)]\n",
        "\n",
        "  return output_stop_words\n",
        "\n",
        "  #pass"
      ],
      "metadata": {
        "id": "Q3j9z7UuW3eG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "control_stop_words = stop_words(control_data_tokenized)\n",
        "print(control_stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-54ZwNGXHzbF",
        "outputId": "0c14ad8e-662b-4342-cfc6-f7beddc89365"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'the', 'to', 'and', 'a', 'of', 'my', 'in', 'it', 'is', 'for', 'that', 'this', 'but', 'on', 'you', 'with', 'was', 'have', 'me', 'so', 'be', 'or', 'just', 'if', 'not', 'what', 'like', 'are', 'as', 'at', 'do', 'about', 'up', 'out', 'can', 'all', 'he', 'from', 'we', 'they', 'her', 'how', 'would', 'she', 'get', 'when', 'one', 'an', 'know', 'had', 'there', 'some', 'been', 'will', 'time', 'any', 'because', 'no', 'more', 'am', 'want', 'your', 'has', 'really', 'people', 'now', 'them', 'amp', 'who', 'other', 'only', 'think', 'by', 'even', 'his', 'back', 'much', 'good', 'then', 'him', 'after', 'also', 'feel', 'go', 'going', 'removed', 'new', 'anyone', 'into', 'make', 'got', 'first', 'could', 'their', 'day', 'than', 'were', 'way', 'which']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(data, stop_words):\n",
        "  \"\"\"Remove stop words from dataset\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data (pd dataframe): dataset\n",
        "      stop_words (list[str]): list of stop words\n",
        "      Returns:\n",
        "      data_without_stop_words (pd dataframe): dataset without stop words\n",
        "  \"\"\"\n",
        "  data_without_stop_words = data.copy()\n",
        "  data_without_stop_words['tokenized_text'] = data_without_stop_words['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "  return data_without_stop_words"
      ],
      "metadata": {
        "id": "fyjG5kgdHbqZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "control_data_without_stop_words = remove_stop_words(control_data_tokenized, control_stop_words)\n",
        "symptom_data_without_stop_words = remove_stop_words(symptom_data_tokenized, control_stop_words)"
      ],
      "metadata": {
        "id": "b6ZQnm5UmDYf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(control_data_without_stop_words.head())\n",
        "print(symptom_data_without_stop_words.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlIVXlOaw52j",
        "outputId": "4a405ef6-349a-42b5-bf03-02ada134861d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text                author  \\\n",
            "315            Man, I do love me some Bandicoot crash.    BuddermanTheAmazing   \n",
            "651   How good is this PC for my 700-750$ budget? Wa...   WildernessExploring   \n",
            "730   When is the price of gpus going down? I know t...  NeighborhoodPizzaGuy   \n",
            "1354  Our service is not available in your area. Hey...               xDEDANx   \n",
            "1598                                               Wow            baby_kicked   \n",
            "\n",
            "                 subreddit  created_utc     date  \\\n",
            "315        crappyoffbrands   1499236239  2017-07   \n",
            "651             buildmeapc   1501296261  2017-07   \n",
            "730           pcmasterrace   1500082729  2017-07   \n",
            "1354                  njpw   1499941432  2017-07   \n",
            "1598  indianpeoplefacebook   1500924182  2017-07   \n",
            "\n",
            "                                         tokenized_text  \n",
            "315                       [man, love, bandicoot, crash]  \n",
            "651   [pc, budget, gaming, high, ultra, settings, th...  \n",
            "730   [price, gpus, down, bitcoin, mining, trying, s...  \n",
            "1354  [our, service, available, area, hey, totally, ...  \n",
            "1598                                              [wow]  \n",
            "                                                   text         author  \\\n",
            "1640  Would you Rather Be Superman Or Have a Massive...        torero5   \n",
            "3187  I have basically everything I’ve really wanted...    Wandertramp   \n",
            "3842  how do i cope with defeat and failure? so i ju...  that_one_kid9   \n",
            "4273  How could I be bored with so much work to do? ...        Sraktai   \n",
            "5346  How to eat decent meals I have been struggling...      MrAfr1can   \n",
            "\n",
            "              subreddit  created_utc     date  \\\n",
            "1640           selfhelp   1514386325  ression   \n",
            "3187           selfhelp   1508588646  ression   \n",
            "3842           selfhelp   1503614431  ression   \n",
            "4273  whatsbotheringyou   1507628776  ression   \n",
            "5346           selfhelp   1500861393  ression   \n",
            "\n",
            "                                         tokenized_text  \n",
            "1640                 [rather, superman, massive, harem]  \n",
            "3187  [basically, everything, ve, wanted, life, s, e...  \n",
            "3842  [cope, defeat, failure, gym, lost, game, 21, b...  \n",
            "4273  [bored, work, hard, relaxing, house, im, swimm...  \n",
            "5346  [eat, decent, meals, struggling, hunger, latel...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data up into several pieces and run methods\n",
        "# I just realized I don't think I actually need this\n",
        "\n",
        "def split_data(data, n):\n",
        "  \"\"\"Split data into n pieces\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data (pd dataframe): dataset\n",
        "      n (int): number of pieces to split data into\n",
        "      Returns:\n",
        "      data_pieces (list[pd dataframe]): list of n pieces of data\n",
        "  \"\"\"\n",
        "\n",
        "  return np.array_split(data, n)\n",
        "\n",
        "def process_chunks(data_chunks):\n",
        "  \"\"\"Process chunks of data using the preprocessing methods I wrote above\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data_chunks (list[pd dataframe]): list of n pieces of data\n",
        "      Returns:\n",
        "      data_without_stop_words (list[pd dataframe]): list of n pieces of data without stop words\n",
        "  \"\"\"\n",
        "\n",
        "  processed_chunks = []\n",
        "  for chunk in data_chunks:\n",
        "    chunk = tokenize(chunk)\n",
        "    chunk = further_cleaning(chunk)\n",
        "    chunk = remove_stop_words(chunk, control_stop_words)\n",
        "    processed_chunks.append(chunk)\n",
        "\n",
        "  return processed_chunks\n",
        "\n",
        "def combine_chunks(data_chunks):\n",
        "  \"\"\"Combine chunks of data into one dataset\"\"\"\n",
        "  \"\"\"Args:\n",
        "      data_chunks (list[pd dataframe]): list of n pieces of data\n",
        "      Returns:\n",
        "      combined_data (pd dataframe): combined dataset\n",
        "  \"\"\"\n",
        "\n",
        "  combined_data = pd.concat(data_chunks)\n",
        "  return combined_data"
      ],
      "metadata": {
        "id": "w6PG0hnt0vhJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_original_data = split_data(data, 10)\n",
        "cleaned_original_data = process_chunks(split_original_data)\n",
        "combined_original_data = combine_chunks(cleaned_original_data)\n",
        "\n",
        "# I don't think I actually need this, nvm!"
      ],
      "metadata": {
        "id": "-h_rlUHC2vQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data_without_stop_words = pd.concat([control_data_without_stop_words, symptom_data_without_stop_words])\n",
        "print(combined_data_without_stop_words.head())\n",
        "\n",
        "# I spent an embarrassingly long time trying to figure out how to combine two\n",
        "# gensim Dictionary objects together into another Dictionary before I realized\n",
        "# I could just do it up here beforehand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2T0g247_X8a",
        "outputId": "34ec2edd-c57b-40a7-da75-3b71346ac2ef"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text                author  \\\n",
            "315            Man, I do love me some Bandicoot crash.    BuddermanTheAmazing   \n",
            "651   How good is this PC for my 700-750$ budget? Wa...   WildernessExploring   \n",
            "730   When is the price of gpus going down? I know t...  NeighborhoodPizzaGuy   \n",
            "1354  Our service is not available in your area. Hey...               xDEDANx   \n",
            "1598                                               Wow            baby_kicked   \n",
            "\n",
            "                 subreddit  created_utc     date  \\\n",
            "315        crappyoffbrands   1499236239  2017-07   \n",
            "651             buildmeapc   1501296261  2017-07   \n",
            "730           pcmasterrace   1500082729  2017-07   \n",
            "1354                  njpw   1499941432  2017-07   \n",
            "1598  indianpeoplefacebook   1500924182  2017-07   \n",
            "\n",
            "                                         tokenized_text  \n",
            "315                       [man, love, bandicoot, crash]  \n",
            "651   [pc, budget, gaming, high, ultra, settings, th...  \n",
            "730   [price, gpus, down, bitcoin, mining, trying, s...  \n",
            "1354  [our, service, available, area, hey, totally, ...  \n",
            "1598                                              [wow]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ],
      "metadata": {
        "id": "U4I37U1SXAEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We highly recommend you using the LdaMulticore interface, but feel free to use any other implementations if you prefer.\n",
        "# from gensim.models import LdaMulticore\n",
        "\n",
        "# TODO: Your LDA code!\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "\n",
        "dictionary = Dictionary(combined_data_without_stop_words['tokenized_text'])\n",
        "print(f\"Number of unique tokens: {len(dictionary)}\")\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in control_data_without_stop_words['tokenized_text']]\n",
        "print(f\"Corpus size: {len(dictionary)}\")\n",
        "\n",
        "dictionary.save('control_dict.gensim')\n",
        "with open('control_corpus.pkl', 'wb') as f:\n",
        "  pickle.dump(corpus, f)\n",
        "\n",
        "def train_lda_model(dictionary, corpus, num_topics=200, alpha=5, passes=10, workers=4):\n",
        "    \"\"\"Train LDA model\"\"\"\n",
        "    \"\"\"Args:\n",
        "        dictionary: gensim dictionary that maps words to IDs\n",
        "        corpus: BoW corpus\n",
        "        num_topics: number of topics\n",
        "        alpha: hyperparameter for sparsity of topics per doc\n",
        "        passes: number of passes\n",
        "        workers: number of workers\n",
        "        Returns:\n",
        "        lda_model: trained LDA model\n",
        "        topics: list of topics\n",
        "    \"\"\"\n",
        "\n",
        "    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha=alpha/num_topics, passes=passes, workers=workers)\n",
        "    topic_features = []\n",
        "    for doc in corpus:\n",
        "      topics = lda_model.get_document_topics(doc)\n",
        "      topic_features.append([prob for topic, prob in topics])\n",
        "\n",
        "    topic_features = pd.DataFrame(topic_features)\n",
        "\n",
        "    return lda_model, topics\n"
      ],
      "metadata": {
        "id": "xf3surfWXH-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4885206f-4073-4d5b-942a-3fa9ccd56696"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 23551\n",
            "Corpus size: 23551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model, topics = train_lda_model(dictionary, corpus)\n",
        "\n",
        "lda_model.save('lda_model.gensim')"
      ],
      "metadata": {
        "id": "eOofD5oDzwa6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda_model.print_topics())\n",
        "\n",
        "# not too sure how to interpret this but ok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9fmvTY6AbsU",
        "outputId": "3c7ccf51-b12e-4f6c-bce0-01573eb9aa13"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(137, '0.013*\"off\" + 0.012*\"10\" + 0.011*\"kits\" + 0.011*\"testing\" + 0.011*\"drug\" + 0.011*\"400\" + 0.008*\"owner\" + 0.007*\"use\" + 0.007*\"moments\" + 0.007*\"needed\"'), (126, '0.010*\"does\" + 0.010*\"need\" + 0.009*\"effect\" + 0.009*\"winter\" + 0.007*\"fireworks\" + 0.006*\"realize\" + 0.006*\"found\" + 0.006*\"genius\" + 0.005*\"two\" + 0.005*\"anything\"'), (31, '0.011*\"2meirl4meirl\" + 0.009*\"myself\" + 0.008*\"each\" + 0.008*\"why\" + 0.007*\"friendship\" + 0.007*\"rings\" + 0.006*\"person\" + 0.006*\"guys\" + 0.006*\"smart\" + 0.006*\"these\"'), (192, '0.013*\"police\" + 0.012*\"parents\" + 0.011*\"few\" + 0.010*\"nmum\" + 0.010*\"years\" + 0.008*\"never\" + 0.008*\"family\" + 0.008*\"life\" + 0.008*\"why\" + 0.008*\"school\"'), (54, '0.024*\"cheese\" + 0.021*\"month\" + 0.015*\"chocolate\" + 0.015*\"consuming\" + 0.015*\"describe\" + 0.012*\"mostly\" + 0.009*\"ingram\" + 0.009*\"sg\" + 0.009*\"craving\" + 0.009*\"damn\"'), (144, '0.044*\"watch\" + 0.019*\"episode\" + 0.012*\"question\" + 0.012*\"morning\" + 0.012*\"questions\" + 0.011*\"use\" + 0.011*\"friends\" + 0.010*\"boyfriend\" + 0.010*\"card\" + 0.010*\"verizon\"'), (38, '0.053*\"music\" + 0.045*\"red\" + 0.033*\"playlist\" + 0.022*\"july\" + 0.020*\"coil\" + 0.020*\"party\" + 0.017*\"pen\" + 0.016*\"fourth\" + 0.013*\"kief\" + 0.013*\"forward\"'), (172, '0.021*\"irl\" + 0.018*\"song\" + 0.009*\"use\" + 0.008*\"help\" + 0.008*\"faith\" + 0.008*\"g\" + 0.008*\"tech\" + 0.006*\"things\" + 0.006*\"megabro\" + 0.006*\"windows\"'), (70, '0.024*\"us\" + 0.010*\"participants\" + 0.010*\"military\" + 0.009*\"study\" + 0.009*\"survey\" + 0.009*\"personality\" + 0.008*\"psychology\" + 0.008*\"university\" + 0.008*\"down\" + 0.007*\"must\"'), (196, '0.023*\"subreddit\" + 0.010*\"shower\" + 0.008*\"baby\" + 0.007*\"try\" + 0.006*\"happened\" + 0.006*\"etc\" + 0.006*\"here\" + 0.006*\"house\" + 0.006*\"animations\" + 0.005*\"too\"'), (35, '0.019*\"learn\" + 0.012*\"japanese\" + 0.011*\"why\" + 0.008*\"english\" + 0.007*\"video\" + 0.007*\"same\" + 0.007*\"official\" + 0.006*\"wife\" + 0.006*\"show\" + 0.006*\"relationships\"'), (180, '0.236*\"game\" + 0.030*\"play\" + 0.020*\"true\" + 0.017*\"tonight\" + 0.016*\"favourite\" + 0.015*\"best\" + 0.014*\"beat\" + 0.014*\"games\" + 0.012*\"ending\" + 0.010*\"top\"'), (59, '0.011*\"same\" + 0.010*\"5\" + 0.010*\"days\" + 0.010*\"bf\" + 0.008*\"said\" + 0.007*\"maybe\" + 0.007*\"use\" + 0.006*\"does\" + 0.006*\"dp\" + 0.005*\"still\"'), (191, '0.061*\"nbsp\" + 0.018*\"used\" + 0.012*\"palette\" + 0.008*\"eyes\" + 0.008*\"1x\" + 0.007*\"eyeshadow\" + 0.007*\"foundation\" + 0.007*\"swatched\" + 0.007*\"lips\" + 0.006*\"pink\"'), (110, '0.034*\"contact\" + 0.032*\"moving\" + 0.027*\"ex\" + 0.024*\"tips\" + 0.019*\"weekend\" + 0.017*\"last\" + 0.016*\"ruin\" + 0.016*\"nearly\" + 0.015*\"remain\" + 0.013*\"doing\"'), (45, '0.023*\"natural\" + 0.016*\"bird\" + 0.013*\"pweep\" + 0.013*\"ads\" + 0.013*\"malware\" + 0.013*\"charger\" + 0.011*\"page\" + 0.010*\"focus\" + 0.010*\"does\" + 0.010*\"adblocker\"'), (71, '0.046*\"price\" + 0.032*\"internet\" + 0.015*\"galaxy\" + 0.011*\"island\" + 0.011*\"independent\" + 0.011*\"least\" + 0.011*\"series\" + 0.011*\"quick\" + 0.011*\"breakfast\" + 0.010*\"replacement\"'), (118, '0.015*\"bonus\" + 0.010*\"animals\" + 0.010*\"ocean\" + 0.009*\"damage\" + 0.009*\"its\" + 0.008*\"hawk\" + 0.008*\"owl\" + 0.008*\"music\" + 0.008*\"individual\" + 0.008*\"mods\"'), (40, '0.010*\"sound\" + 0.010*\"final\" + 0.009*\"1\" + 0.009*\"oats\" + 0.009*\"milk\" + 0.009*\"articles\" + 0.008*\"protein\" + 0.008*\"salt\" + 0.007*\"2\" + 0.007*\"characters\"'), (85, '0.015*\"gear\" + 0.014*\"die\" + 0.012*\"flash\" + 0.011*\"hug\" + 0.011*\"fill\" + 0.009*\"jack\" + 0.008*\"whenever\" + 0.008*\"playing\" + 0.008*\"guys\" + 0.008*\"earn\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for symptom in tokenized_symptom_data:\n",
        "  symptom_corpus = [dictionary.doc2bow(text) for text in tokenized_symptom_data[symptom]['tokenized_text']]\n",
        "  symptom_topic_features = []\n",
        "  for doc in symptom_corpus:\n",
        "    topics = lda_model.get_document_topics(doc, minimum_probability=0.0000)\n",
        "    symptom_topic_features.append([prob for topic, prob in topics])\n",
        "\n",
        "  symptom_topic_features = pd.DataFrame(symptom_topic_features, columns=[f'topic_{i}' for i in range(200)])\n",
        "  symptom_topic_features.to_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_topic_features.pkl')\n",
        "\n",
        "control_corpus = [dictionary.doc2bow(text) for text in control_data_without_stop_words['tokenized_text']]\n",
        "control_topic_features = []\n",
        "for doc in control_corpus:\n",
        "  topics = lda_model.get_document_topics(doc, minimum_probability=0.0000)\n",
        "  control_topic_features.append([prob for topic, prob in topics])\n",
        "\n",
        "control_topic_features = pd.DataFrame(control_topic_features, columns=[f'topic_{i}' for i in range(200)])\n",
        "control_topic_features.to_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_topic_features.pkl')"
      ],
      "metadata": {
        "id": "kdH8RsX9Ar2A"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symptom_topic_features_df = pd.read_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/anxiety_topic_features.pkl')\n",
        "print(symptom_topic_features_df.head())\n",
        "print(symptom_topic_features.shape)\n",
        "print(control_topic_features.head())\n",
        "print(control_topic_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tFD8lsYPaE0",
        "outputId": "99f84c09-1750-4886-a4de-af99be29a54d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
            "0  0.044271  0.000313  0.000313  0.000313  0.000313  0.000313  0.000313   \n",
            "1  0.002778  0.002778  0.002778  0.002778  0.002778  0.002778  0.002778   \n",
            "2  0.002273  0.081088  0.002273  0.002273  0.002273  0.002273  0.002273   \n",
            "3  0.000099  0.000099  0.000099  0.000099  0.000099  0.000099  0.000099   \n",
            "4  0.001191  0.001191  0.001191  0.001191  0.001191  0.135397  0.001191   \n",
            "\n",
            "    topic_7   topic_8   topic_9  ...  topic_190  topic_191  topic_192  \\\n",
            "0  0.000313  0.000313  0.000313  ...   0.000313   0.000313   0.134790   \n",
            "1  0.002778  0.002778  0.002778  ...   0.002778   0.002778   0.002778   \n",
            "2  0.155012  0.002273  0.002273  ...   0.002273   0.002273   0.002273   \n",
            "3  0.000099  0.000099  0.000099  ...   0.000099   0.000099   0.137941   \n",
            "4  0.001191  0.001191  0.001191  ...   0.001191   0.001191   0.001191   \n",
            "\n",
            "   topic_193  topic_194  topic_195  topic_196  topic_197  topic_198  topic_199  \n",
            "0   0.000313   0.000313   0.000313   0.000313   0.000313   0.000313   0.000313  \n",
            "1   0.002778   0.002778   0.002778   0.002778   0.002778   0.002778   0.002778  \n",
            "2   0.002273   0.002273   0.002273   0.002273   0.002273   0.002273   0.002273  \n",
            "3   0.000099   0.000099   0.000099   0.000099   0.007221   0.000099   0.000099  \n",
            "4   0.001191   0.001191   0.106867   0.204122   0.001191   0.001191   0.001191  \n",
            "\n",
            "[5 rows x 200 columns]\n",
            "(1805, 200)\n",
            "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
            "0  0.002778  0.002778  0.002778  0.002778  0.002778  0.002778  0.002778   \n",
            "1  0.001786  0.001786  0.001786  0.001786  0.001786  0.001786  0.001786   \n",
            "2  0.001563  0.001563  0.001563  0.001563  0.001563  0.001563  0.001563   \n",
            "3  0.000676  0.000676  0.000676  0.000676  0.000676  0.000676  0.000676   \n",
            "4  0.004167  0.004167  0.004167  0.004167  0.004167  0.004167  0.004167   \n",
            "\n",
            "    topic_7   topic_8   topic_9  ...  topic_190  topic_191  topic_192  \\\n",
            "0  0.002778  0.002778  0.002778  ...   0.002778   0.002778   0.002778   \n",
            "1  0.001786  0.001786  0.001786  ...   0.001786   0.001786   0.001786   \n",
            "2  0.001563  0.001563  0.001563  ...   0.001563   0.001563   0.090927   \n",
            "3  0.000676  0.000676  0.000676  ...   0.000676   0.000676   0.000676   \n",
            "4  0.004167  0.004167  0.004167  ...   0.004167   0.004167   0.004167   \n",
            "\n",
            "   topic_193  topic_194  topic_195  topic_196  topic_197  topic_198  topic_199  \n",
            "0   0.002778   0.002778   0.002778   0.002778   0.002778   0.002778   0.002778  \n",
            "1   0.001786   0.001786   0.001786   0.001786   0.001786   0.001786   0.001786  \n",
            "2   0.001563   0.001563   0.001563   0.001563   0.001563   0.001563   0.001563  \n",
            "3   0.000676   0.000676   0.000676   0.000676   0.000676   0.000676   0.000676  \n",
            "4   0.004167   0.004167   0.004167   0.004167   0.004167   0.004167   0.004167  \n",
            "\n",
            "[5 rows x 200 columns]\n",
            "(4369, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa Embeddings"
      ],
      "metadata": {
        "id": "E0-97hsVXNkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Your RoBERTa code!\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "model = AutoModel.from_pretrained(\"distilroberta-base\", output_hidden_states=True)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def get_roberta_embeddings(text_list, tokenizer, model, layer_index=5, max_length=512, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "  \"\"\"Get RoBERTa embeddings\"\"\"\n",
        "  \"\"\"Args:\n",
        "      text_list: list of strings\n",
        "      tokenizer: tokenizer\n",
        "      model: model\n",
        "      layer_index: index of layer to extract embeddings from\n",
        "      max_length: max length of tokens\n",
        "      device: device\n",
        "      Returns:\n",
        "      embeddings: list of embeddings\n",
        "  \"\"\"\n",
        "  model.to(device)\n",
        "\n",
        "  embeddings = []\n",
        "\n",
        "  for text in text_list:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, max_length=max_length, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "    hidden_states = outputs.hidden_states\n",
        "    layer_hidden_state = hidden_states[layer_index]\n",
        "\n",
        "    batch_embeddings = layer_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    embeddings.append(batch_embeddings)\n",
        "\n",
        "  return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "blx1SWVMXYDp"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "control_texts = control_data_without_stop_words['tokenized_text'].apply(lambda x: ' '.join(x)).tolist()\n",
        "for symptom in tokenized_symptom_data:\n",
        "  symptom_texts = tokenized_symptom_data[symptom]['tokenized_text'].apply(lambda x: ' '.join(x)).tolist()\n",
        "  symptom_embeddings = get_roberta_embeddings(symptom_texts, tokenizer, model)\n",
        "  np.save(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_roberta_embeddings.npy', symptom_embeddings)\n",
        "\n",
        "control_embeddings = get_roberta_embeddings(control_texts, tokenizer, model)\n",
        "\n",
        "\n",
        "np.save('control_embeddings.npy', control_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "wu4BhNiTdpS0",
        "outputId": "1716ca48-8369-4e03-fdfc-91f3ac755ebe"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-c7bf0f1cdb6f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msymptom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_symptom_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msymptom_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_symptom_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymptom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0msymptom_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_roberta_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymptom_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom}_roberta_embeddings.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymptom_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-7a4bd02ef6ca>\u001b[0m in \u001b[0;36mget_roberta_embeddings\u001b[0;34m(text_list, tokenizer, model, layer_index, max_length, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 )\n\u001b[1;32m    630\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    632\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    521\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         )\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "rDWxuF2jXtwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(X, y, symptom_name):\n",
        "  \"\"\"\n",
        "  Here's the basic structure of the main block! It should run\n",
        "  5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n",
        "  performance.\n",
        "  \"\"\"\n",
        "  \"\"\"Args:\n",
        "      X (np array): features\n",
        "      y (np array): labels\n",
        "      symptom_name (str): name of the symptom\n",
        "      Returns:\n",
        "      None\n",
        "  \"\"\"\n",
        "\n",
        "  rf_classifier = RandomForestClassifier()\n",
        "  cv = KFold(n_splits=5, shuffle=True)\n",
        "  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "\n",
        "  # TODO: Print your training and testing scores!\n",
        "\n",
        "  print(f\"Results for symptom: {symptom_name}\")\n",
        "  print(f\"Training AUC Scores: {results['train_score']}\")\n",
        "  print(f\"Mean Training AUC: {results['train_score'].mean():.4f}\")\n",
        "  print(f\"Testing AUC Scores: {results['test_score']}\")\n",
        "  print(f\"Mean Testing AUC: {results['test_score'].mean():.4f}\\n\")\n",
        "\n",
        "\n",
        "  #pass"
      ],
      "metadata": {
        "id": "koTBPhcDXujb"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for symptom_name in symptoms_to_subreddits:\n",
        "  symptom_topic_features_df = pd.read_pickle(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom_name}_topic_features.pkl')\n",
        "  control_features_df = pd.read_pickle('drive/MyDrive/Colab Notebooks/CS1460 final project/control_topic_features.pkl')\n",
        "\n",
        "  control_features_df['label'] = 0\n",
        "  symptom_topic_features_df['label'] = 1\n",
        "\n",
        "  combined_features_df = pd.concat([control_features_df, symptom_topic_features_df])\n",
        "\n",
        "  X = combined_features_df.drop(columns=['label']).values\n",
        "  y = combined_features_df['label'].values\n",
        "\n",
        "  print(f\"Evaluating symptom: {symptom_name}\")\n",
        "  main(X, y, symptom_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haLltE_KTGwd",
        "outputId": "13372e60-3dfa-40eb-e6bb-3397d3f6b563"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating symptom: anger\n",
            "Results for symptom: anger\n",
            "Training AUC Scores: [0.9986838  0.99899571 0.99878624 0.99875659 0.99904192]\n",
            "Mean Training AUC: 0.9989\n",
            "Testing AUC Scores: [0.94184957 0.93227013 0.94753973 0.94392562 0.93500393]\n",
            "Mean Testing AUC: 0.9401\n",
            "\n",
            "Evaluating symptom: anhedonia\n",
            "Results for symptom: anhedonia\n",
            "Training AUC Scores: [0.99951475 0.99948813 0.99953167 0.99957816 0.99954374]\n",
            "Mean Training AUC: 0.9995\n",
            "Testing AUC Scores: [0.97204433 0.97082255 0.96928778 0.97403482 0.97245053]\n",
            "Mean Testing AUC: 0.9717\n",
            "\n",
            "Evaluating symptom: anxiety\n",
            "Results for symptom: anxiety\n",
            "Training AUC Scores: [0.99981409 0.99986978 0.99984216 0.99984042 0.99982361]\n",
            "Mean Training AUC: 0.9998\n",
            "Testing AUC Scores: [0.97142096 0.971807   0.97149301 0.97121464 0.96634676]\n",
            "Mean Testing AUC: 0.9705\n",
            "\n",
            "Evaluating symptom: concentration_deficit\n",
            "Results for symptom: concentration_deficit\n",
            "Training AUC Scores: [1. 1. 1. 1. 1.]\n",
            "Mean Training AUC: 1.0000\n",
            "Testing AUC Scores: [0.45995423 0.7076659  0.45714286 0.810042   0.45933562]\n",
            "Mean Testing AUC: 0.5788\n",
            "\n",
            "Evaluating symptom: disordered_eating\n",
            "Results for symptom: disordered_eating\n",
            "Training AUC Scores: [0.99934436 0.99924511 0.99911317 0.99922087 0.9992245 ]\n",
            "Mean Training AUC: 0.9992\n",
            "Testing AUC Scores: [0.96096918 0.97332605 0.96252865 0.96713542 0.96796971]\n",
            "Mean Testing AUC: 0.9664\n",
            "\n",
            "Evaluating symptom: fatigue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 376, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
            "    return _average_binary_score(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\", line 76, in _average_binary_score\n",
            "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
            "    raise ValueError(\n",
            "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 214, in _get_response_values\n",
            "    y_pred = _process_predict_proba(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 51, in _process_predict_proba\n",
            "    raise ValueError(\n",
            "ValueError: Got predict_proba of shape (874, 1), but need classifier with two classes.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 214, in _get_response_values\n",
            "    y_pred = _process_predict_proba(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_response.py\", line 51, in _process_predict_proba\n",
            "    raise ValueError(\n",
            "ValueError: Got predict_proba of shape (3496, 1), but need classifier with two classes.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for symptom: fatigue\n",
            "Training AUC Scores: [ 1.  1.  1.  1. nan]\n",
            "Mean Training AUC: nan\n",
            "Testing AUC Scores: [nan nan nan nan nan]\n",
            "Mean Testing AUC: nan\n",
            "\n",
            "Evaluating symptom: loneliness\n",
            "Results for symptom: loneliness\n",
            "Training AUC Scores: [0.99957846 0.99952324 0.99939676 0.99948642 0.9994633 ]\n",
            "Mean Training AUC: 0.9995\n",
            "Testing AUC Scores: [0.9177197  0.91086898 0.90765183 0.92050245 0.90432474]\n",
            "Mean Testing AUC: 0.9122\n",
            "\n",
            "Evaluating symptom: sad_mood\n",
            "Results for symptom: sad_mood\n",
            "Training AUC Scores: [0.99898535 0.99928234 0.99899907 0.99929781 0.99934273]\n",
            "Mean Training AUC: 0.9992\n",
            "Testing AUC Scores: [0.91433293 0.90557874 0.89420079 0.89631076 0.90149045]\n",
            "Mean Testing AUC: 0.9024\n",
            "\n",
            "Evaluating symptom: self_loathing\n",
            "Results for symptom: self_loathing\n",
            "Training AUC Scores: [0.99947343 0.99940909 0.99946443 0.99926076 0.99950263]\n",
            "Mean Training AUC: 0.9994\n",
            "Testing AUC Scores: [0.93389802 0.93086735 0.94054409 0.9376834  0.93757002]\n",
            "Mean Testing AUC: 0.9361\n",
            "\n",
            "Evaluating symptom: sleep_problem\n",
            "Results for symptom: sleep_problem\n",
            "Training AUC Scores: [0.99950903 0.99946931 0.99954863 0.99952115 0.99955249]\n",
            "Mean Training AUC: 0.9995\n",
            "Testing AUC Scores: [0.97398605 0.97469707 0.98343685 0.98056708 0.97477241]\n",
            "Mean Testing AUC: 0.9775\n",
            "\n",
            "Evaluating symptom: somatic_complaint\n",
            "Results for symptom: somatic_complaint\n",
            "Training AUC Scores: [0.99950797 0.99959583 0.99949279 0.99955351 0.99957926]\n",
            "Mean Training AUC: 0.9995\n",
            "Testing AUC Scores: [0.95090379 0.95357741 0.94757291 0.95541125 0.95871837]\n",
            "Mean Testing AUC: 0.9532\n",
            "\n",
            "Evaluating symptom: suicidal_thoughts_and_attempts\n",
            "Results for symptom: suicidal_thoughts_and_attempts\n",
            "Training AUC Scores: [0.99976477 0.99982552 0.99983896 0.99979541 0.99985469]\n",
            "Mean Training AUC: 0.9998\n",
            "Testing AUC Scores: [0.96355508 0.96330322 0.96448592 0.96063634 0.96865004]\n",
            "Mean Testing AUC: 0.9641\n",
            "\n",
            "Evaluating symptom: worthlessness\n",
            "Results for symptom: worthlessness\n",
            "Training AUC Scores: [0.99797326 0.99800599 0.99796751 0.99818655 0.99787348]\n",
            "Mean Training AUC: 0.9980\n",
            "Testing AUC Scores: [0.82127522 0.84891441 0.86142302 0.86737004 0.85548321]\n",
            "Mean Testing AUC: 0.8509\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for symptom_name in symptoms_to_subreddits:\n",
        "  symptom_roberta_embeddings = np.load(f'drive/MyDrive/Colab Notebooks/CS1460 final project/{symptom_name}_roberta_embeddings.npy')\n",
        "  control_roberta_embeddings = np.load(f'drive/MyDrive/Colab Notebooks/CS1460 final project/control_embeddings.npy')\n",
        "\n",
        "  X = np.concatenate([symptom_roberta_embeddings, control_roberta_embeddings])\n",
        "  y = np.concatenate([np.ones(symptom_roberta_embeddings.shape[0]), np.zeros(control_roberta_embeddings.shape[0])])\n",
        "\n",
        "  main(X, y, symptom_name)"
      ],
      "metadata": {
        "id": "ZDK2zP1Tj33W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}